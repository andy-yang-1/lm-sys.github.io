{"pageProps":{"posts":[{"slug":"2023-05-03-arena","frontmatter":{"title":"Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings","author":"Lianmin Zheng*, Ying Sheng*, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, Ion Stoica","date":"May 3, 2023","previewImg":"/images/blog/arena/cover.png"},"content":"\r\nWe present Chatbot Arena, a benchmark platform for large language models (LLMs) that features anonymous, randomized battles in a crowdsourced manner. In this blog post, we are releasing our initial results and a leaderboard based on the Elo rating system, which is a widely-used rating system in chess and other competitive games. We invite the entire community to join this effort by contributing new models and evaluating them by asking questions and voting for your favorite answer.\r\n\r\n<style>\r\nth {text-align: left}\r\ntd {text-align: left}\r\n</style>\r\n\r\n<br>\r\n<p style=\"color:gray; text-align: center;\">Table 1. Elo ratings of popular open-source large language models.</p>\r\n<table style=\"display: flex; justify-content: center;\" align=\"left\" >\r\n<tbody>\r\n<tr>\r\n<th>Rank</th> <th>Model</th> <th>Elo Rating</th> <th>Description</th>\r\n</tr>\r\n<tr>\r\n<td>1</td> <td>ðŸ¥‡ <a href=\"https://vicuna.lmsys.org\" target=\"_blank\">vicuna-13b</a></td> <td>1169</td> <td>a chat assistant fine-tuned from LLaMA on user-shared conversations by LMSYS</td>\r\n</tr>\r\n<tr>\r\n<td>2</td> <td>ðŸ¥ˆ <a href=\"https://bair.berkeley.edu/blog/2023/04/03/koala\" target=\"_blank\">koala-13b</a></td> <td>1082</td> <td>a dialogue model for academic research by BAIR</td>\r\n</tr>\r\n<tr>\r\n<td>3</td> <td>ðŸ¥‰ <a href=\"https://open-assistant.io\" target=\"_blank\">oasst-pythia-12b</a></td> <td>1065</td> <td>an Open Assistant for everyone by LAION</td>\r\n</tr>\r\n<tr>\r\n<td>4</td> <td><a href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\" target=\"_blank\">alpaca-13b</a></td> <td>1008</td> <td>a model fine-tuned from LLaMA on instruction-following demonstrations by Stanford</td>\r\n</tr>\r\n<tr>\r\n<td>5</td> <td><a href=\"https://chatglm.cn/blog\" target=\"_blank\">chatglm-6b</a></td> <td>985</td> <td>an open bilingual dialogue language model by Tsinghua University</td>\r\n</tr>\r\n<tr>\r\n<td>6</td> <td><a href=\"https://huggingface.co/lmsys/fastchat-t5-3b-v1.0\" target=\"_blank\">fastchat-t5-3b</a></td> <td>951</td> <td>a chat assistant fine-tuned from FLAN-T5 by LMSYS</td>\r\n</tr>\r\n<tr>\r\n<td>7</td> <td><a href=\"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\" target=\"_blank\">dolly-v2-12b</a></td> <td>944</td> <td>an instruction-tuned open large language model by Databricks</td>\r\n</tr>\r\n<tr>\r\n<td>8</td> <td><a href=\"https://arxiv.org/abs/2302.13971\" target=\"_blank\">llama-13b</a></td> <td>932</td> <td>open and efficient foundation language models by Meta</td>\r\n</tr>\r\n<tr>\r\n<td>9</td> <td><a href=\"https://github.com/stability-AI/stableLM\" target=\"_blank\">stablelm-tuned-alpha-7b</a></td> <td>858</td> <td>Stability AI language models</td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n\r\n&shy;\r\n\r\nTable 1 displays the Elo ratings of eight popular models, which are based on the 4.7K voting data and calculations shared in this [notebook](https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing). You can also try the voting [demo](https://arena.lmsys.org) and see more about the [leaderboard](https://leaderboard.lmsys.org).\r\n\r\n<img src=\"/images/blog/arena/chat_demo.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"></img>\r\n<p style=\"color:gray; text-align: center;\">Figure 1. The side-by-side chatting and voting interface.</p>\r\n\r\n## Introduction\r\nFollowing the great success of ChatGPT, there has been a proliferation of open-source large language models that are finetuned to follow instructions. These models are capable of providing valuable assistance in response to usersâ€™ questions/prompts. Notable examples include Alpaca and Vicuna, based on LLaMA, and OpenAssistant and Dolly, based on Pythia.\r\n\r\nDespite the constant release of new models every week, the community faces a challenge in benchmarking these models effectively. Benchmarking LLM assistants is extremely challenging because the problems can be open-ended, and it is very difficult to write a program to automatically evaluate the response quality. In this case, we typically have to resort to human evaluation based on pairwise comparison.\r\n\r\nThere are some desired properties for a good benchmark system based on pairwise comparison.\r\n- **Scalability**. The system should scale to a large number of models when it is not feasible to collect sufficient data for all possible model pairs.\r\n- **Incrementality**. The system should be able to evaluate a new model using a relatively small number of trials.\r\n- **Unique order**. The system should provide a unique order for all models. Given any two models, we should be able to tell which ranks higher or whether they are tied.\r\n\r\nExisting LLM benchmark systems rarely satisfy all of these properties. Classical LLM benchmark frameworks, such as [HELM](https://crfm.stanford.edu/helm/latest/) and [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), provide multi-metric measurements for tasks commonly used in academic research. However, they are not based on pairwise comparison and are not effective at evaluating open-ended questions. OpenAI also launched the [evals](https://github.com/openai/evals) project to collect better questions, but this project does not provide ranking mechanisms for all participating models. When we launched our [Vicuna](https://vicuna.lmsys.org/) model, we utilized a GPT-4-based evaluation pipeline, but it does not provide a solution for scalable and incremental ratings.\r\n\r\nIn this blog post, we introduce Chatbot Arena, an LLM benchmark platform featuring anonymous randomized battles in a crowdsourced manner. Chatbot Arena adopts the [Elo rating system](https://en.wikipedia.org/wiki/Elo_rating_system), which is a widely-used rating system in chess and other competitive games. The Elo rating system is promising to provide the desired property mentioned above. \r\n\r\nTo collect data, we launched the arena with several popular open-source LLMs one week ago. In the arena, a user can chat with two anonymous models side-by-side and vote for which one is better. This crowdsourcing way of data collection represents some use cases of LLMs in the wild. A comparison between several evaluation methods is shown in Table 2.\r\n\r\n<br>\r\n<p style=\"color:gray; text-align: center;\">Table 2: Comparison between different evaluation methods.</p>\r\n<table style=\"display: flex; justify-content: center;\">\r\n<tbody>\r\n<tr>\r\n<th></th> <th>HELM / lm-evaluation-harness</th> <th>OpenAI/eval</th> <th>Alpaca Evaluation</th> <th>Vicuna Evaluation</th> <th>Chatbot Arena</th>\r\n</tr>\r\n<tr>\r\n<td>Question Source</td> <td>Academic datasets</td> <td>Mixed</td> <td>Self-instruct evaluation set</td> <td>GPT-4 generated</td> <td>User prompts</td>\r\n</tr>\r\n<tr>\r\n<td>Evaluator</td> <td>Program</td> <td>Program/Model</td> <td>Human</td> <td>GPT-4</td> <td>User</td>\r\n</tr>\r\n<tr>\r\n<td>Metrics</td> <td>Basic metrics </td> <td>Basic metrics</td> <td>Win rate</td> <td>Win rate</td> <td>Elo ratings</td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n\r\n## Data Collection\r\nWe hosted the arena at [https://arena.lmsys.org](https://arena.lmsys.org) with our multi-model serving system, [FastChat](https://github.com/lm-sys/FastChat). When a user enters the arena, they can chat with two anonymous models side-by-side, as shown in Figure 1.\r\nAfter getting responses from the two models, users can continue chatting or vote for the model they think is better. Once a vote is submitted, the model names will be revealed. Users can continue chatting or restart a new battle with two new randomly chosen anonymous models. The platform logs all user interactions. In our analysis, we only use the votes when the model names are hidden.\r\n\r\nThe arena was launched about one week ago and we have collected 4.7k valid anonymous votes since then.  We share some exploratory analysis in this [notebook](https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing) and present a short summary here.\r\n\r\n<img src=\"/images/blog/arena/battle_counts.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%\"></img>\r\n<p style=\"color:gray; text-align: center;\">Figure 2: Battle count of each combination of models</p>\r\n\r\nFigure 2 shows the battles count of each combination of models. When we initially launched the tournament, we had prior information on the likely ranking based on our benchmarks and chose to pair models according to this ranking. We gave preference to what we believed would be strong pairings based on this ranking. However, we later switched to uniform sampling to get better overall coverage of the rankings. Towards the end of the tournament, we also introduced a new model `fastchat-t5-3b`. All of these result in non-uniform model frequency.\r\n\r\n<img src=\"/images/blog/arena/lang_counts.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 80%\"></img>\r\n<p style=\"color:gray; text-align: center;\">Figure 3: Battle counts for the top-15 languages.</p>\r\n\r\nFigure 3 plots the language distribution and shows most user prompts are in English.\r\n\r\n## Elo Rating System\r\nThe [Elo rating system](https://en.wikipedia.org/wiki/Elo_rating_system) is a method for calculating the relative skill levels of players, which has been widely adopted in competitive games and sports. The difference in the ratings between two players serves as a predictor of the outcome of a match. The Elo rating system works well for our case because we have multiple models and we run pairwise battles between them.\r\n\r\n\r\nIf player A has a rating of `R_a` and player B a rating of `R_b`, the exact formula (using the logistic curve with base 10) for the probability of player A winning is\r\n\r\n<img src=\" https://wikimedia.org/api/rest_v1/media/math/render/svg/7c80282e9c95e92d6b210467aab48a8c4c81ef10\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"></img>\r\n\r\nUsing the collected data, we compute the Elo ratings of the models in this [notebook](https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing) and put the main results in Table 1. You are welcome to try the notebook and play with the voting data by yourself. The data only contains voting results without conversation histories because releasing the conversation history will raise concerns such as privacy and toxicity.\r\n\r\n## Pairwise Win Rates\r\nAs a basis for calibration, we also present here the pairwise win rates for each model in the tournament (Figure 4) as well as the predicted pairwise win rate estimated using Elo ratings (Figure 5).\r\nBy comparing the figures, we find the elo ratings can predict win rates relatively well.\r\n\r\n<img src=\"/images/blog/arena/win_fraction.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"></img>\r\n<p style=\"color:gray; text-align: center;\">Figure 4: Fraction of Model A wins for all non-tied A vs. B battles.</p>\r\n\r\n<img src=\"/images/blog/arena/predicted_win_fraction.png\" style=\"display:block; margin-left: auto; margin-right: auto; margin-bottom: auto;\"></img>\r\n<p style=\"color:gray; text-align: center;\">Figure 5: Predicted win rate using Elo ratings for Model A in an A vs. B battle</p>\r\n\r\n## Future Plans\r\nWe plan to work on the following items:\r\n- Add more close-source models (ChatGPT-3.5 is avaiable now in the anonymous Arena)\r\n- Add more open-source models\r\n- Release periodically updated leaderboards (e.g., monthly)\r\n- Implement better sampling algorithms, tournament mechanisms, and serving systems to support a much larger number of models\r\n- Provide fine-grained rankings on different task types.\r\n\r\nWe appreciate any feedback from you to make the arena better.\r\n\r\n## Join Us\r\nWe invite the entire community to join this benchmarking effort by contributing your models and votes for the anonymous models you think provide better answers. You can visit [https://arena.lmsys.org](https://arena.lmsys.org) to vote for better models. If you want to see a specific model in the arena, you can follow this [guide](https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model) to help us add it.\r\n\r\n## Acknowledgment\r\nWe thank other members of the Vicuna team for valuable feedback and MBZUAI for donating compute resources. Additionally, we extend our thanks to Tianjun Zhang and Eric Wallace for their insightful discussions.\r\n\r\n## Links\r\n- Demo: [https://arena.lmsys.org](https://arena.lmsys.org)\r\n- Leaderboard: [https://leaderboard.lmsys.org](https://leaderboard.lmsys.org)\r\n- GitHub: [https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat)\r\n- Colab notebook: [https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing](https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing)\r\n","date":1683072000000},{"slug":"2023-03-30-vicuna","frontmatter":{"title":"Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality","author":"The Vicuna Team","date":"March 30, 2023","previewImg":"/images/blog/vicuna/vicuna.jpeg"},"content":"\r\nWe introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%<sup>*</sup> of cases. The cost of training Vicuna-13B is around $300. The [code](https://github.com/lm-sys/FastChat) and [weights](https://github.com/lm-sys/FastChat#vicuna-weights), along with an online [demo](https://chat.lmsys.org), are publicly available for non-commercial use.\r\n\r\n<img src=\"/images/blog/vicuna/vicuna.jpeg\" style=\"width: 30%; margin-left: auto; margin-right: auto; margin-bottom: auto\"></img>\r\n<p style=\"color:gray; text-align: center;\">Vicuna (generated by stable diffusion 2.1) </p>\r\n\r\n<p style=\"color:gray;\">*According to a fun and non-scientific evaluation with GPT-4. Further rigorous evaluation is needed.</p>\r\n\r\n## How Good is Vicuna?\r\nAfter fine-tuning Vicuna with 70K user-shared ChatGPT conversations, we discover that Vicuna becomes capable of generating more detailed and well-structured answers compared to Alpaca (see examples below), with the quality on par with ChatGPT.\r\n\r\n<style>\r\n.tg  {border-collapse:collapse;border-spacing:0;margin:0px auto;}\r\n.tg td{border-color:#ccc;border-style:solid;border-width:1px;\r\n  overflow:hidden;padding:10px 5px;word-break:normal;}\r\n.tg .tg-head{background-color:#c0c0c0;border-color:#ccc;text-align:left;vertical-align:top;}\r\n.tg .tg-body{text-align:left;vertical-align:top;}\r\n</style>\r\n\r\n<style>\r\n  iframe {\r\n    display: block;\r\n    width: 100%;\r\n    height: 900px;\r\n    border: none;\r\n    overflow: hidden;\r\n  }\r\n</style>\r\n<iframe src=\"/images/blog/vicuna/gpt4eval/index.html\"></iframe>\r\n<hr>\r\n\r\nHowever, evaluating chatbots is never a simple task. \r\nWith recent advancements in GPT-4, we are curious whether its capabilities have reached a human-like level that could enable an automated evaluation framework for benchmark generation and performance assessments. \r\nOur initial finding indicates that GPT-4 can produce highly consistent ranks and detailed assessment when comparing chatbotsâ€™ answers (see above example of GPT-4 judgment).\r\nPreliminary evaluations based on GPT-4, summarized in Figure 1, show that Vicuna achieves 90%<sup>*</sup> capability of Bard/ChatGPT. \r\nWhile this proposed framework shows a potential to automate chatbot assessment, **it is not yet a rigorous approach**. \r\nBuilding an evaluation system for chatbots remains an open question requiring further research. More details are provided in the evaluation section.\r\n\r\n<img src=\"/images/blog/vicuna/chart.svg\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%\"></img>\r\n<p style=\"color:gray; text-align: center;\">Figure 1. Relative Response Quality Assessed by GPT-4*</p>\r\n\r\n## Online Demo\r\nTry the Vicuna-13B demo [here](https://chat.lmsys.org)!\r\n\r\n<!-- Add a video that automatically play -->\r\n<div>\r\n  <a href=\"https://chat.lmsys.org\"  style=\"display: flex; justify-content: center; margin-top: 1em; margin-bottom: 1em;\">\r\n  <video autoplay muted loop src=\"/images/blog/vicuna/demo-narrow.mp4\" type=\"video/mp4\" style=\"width: 70%;\" id=\"demo\">\r\n  </video>\r\n  </a>\r\n</div>\r\n\r\n## Overview\r\nThe rapid advancement of large language models (LLMs) has revolutionized chatbot systems, resulting in unprecedented levels of intelligence as seen in OpenAI's ChatGPT. However, despite its impressive performance, the training and architecture details of ChatGPT remain unclear, hindering research and open-source innovation in this field. Inspired by the Meta LLaMA and Stanford Alpaca project, we introduce Vicuna-13B, an open-source chatbot backed by an enhanced dataset and an easy-to-use, scalable infrastructure. By fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.com, Vicuna-13B has demonstrated competitive performance compared to other open-source models like Stanford Alpaca. This blog post provides a preliminary evaluation of Vicuna-13B's performance and describes its training and serving infrastructure. We also invite the community to interact with our online demo to test the capabilities of this chatbot.\r\n\r\n<img src=\"/images/blog/vicuna/overview.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%\"></img>\r\n<p style=\"color:gray; text-align: center;\">Figure 2. Workflow Overview</p>\r\n\r\nFigure 2 provides an overview of our work. To begin, we collected around 70K conversations from ShareGPT.com, a website where users can share their ChatGPT conversations. Next, we enhanced the training scripts provided by Alpaca to better handle multi-round conversations and long sequences. The training was done with PyTorch FSDP on 8 A100 GPUs in one day. For serving the demo, we implemented a lightweight distributed serving system. We conducted a preliminary evaluation of the model quality by creating a set of 80 diverse questions and utilizing GPT-4 to judge the model outputs. To compare two different models, we combine the outputs from each model into a single prompt for each question. The prompts are then sent to GPT-4, which assesses which model provides better responses. A detailed comparison of LLaMA, Alpaca, ChatGPT, and Vicuna is shown in Table 1 below.\r\n\r\n\r\n<p style=\"color:gray; text-align: center;\">Table 1. Comparison between several notable models</p>\r\n\r\n<table class=\"tg\" style=\"display: flex;justify-content: center;\">\r\n<tbody>\r\n  <tr>\r\n    <td class=\"tg-head\"><span style=\"font-weight:bold;\">Model Name</span></td>\r\n    <td class=\"tg-head\"><span style=\"font-weight:bold;\">LLaMA</span></td>\r\n    <td class=\"tg-head\"><span style=\"font-weight:bold;\">Alpaca</span></td>\r\n    <td class=\"tg-head\"><span style=\"font-weight:bold;\">Vicuna</span></td>\r\n    <td class=\"tg-head\"><span style=\"font-weight:bold;\">Bard/ChatGPT</span></td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-body\">Dataset</td>\r\n    <td class=\"tg-body\">Publicly available datasets<br>(1T token)</td>\r\n    <td class=\"tg-body\">Self-instruct from davinci-003 API<br>(52K samples)</td>\r\n    <td class=\"tg-body\">User-shared conversations<br>(70K samples)</td>\r\n    <td class=\"tg-body\">N/A</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-body\">Training code</td>\r\n    <td class=\"tg-body\">N/A</td>\r\n    <td class=\"tg-body\">Available</td>\r\n    <td class=\"tg-body\">Available</td>\r\n    <td class=\"tg-body\">N/A</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-body\">Evaluation metrics</td>\r\n    <td class=\"tg-body\">Academic benchmark</td>\r\n    <td class=\"tg-body\">Author evaluation</td>\r\n    <td class=\"tg-body\">GPT-4 assessment</td>\r\n    <td class=\"tg-body\">Mixed</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-body\">Training cost<br>(7B)</td>\r\n    <td class=\"tg-body\">82K GPU-hours</td>\r\n    <td class=\"tg-body\">$500 (data) + $100 (training)</td>\r\n    <td class=\"tg-body\">$140 (training)</td>\r\n    <td class=\"tg-body\">N/A</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-body\">Training cost<br>(13B)</td>\r\n    <td class=\"tg-body\">135K GPU-hours</td>\r\n    <td class=\"tg-body\">N/A</td>\r\n    <td class=\"tg-body\">$300 (training)</td>\r\n    <td class=\"tg-body\">N/A</td>\r\n  </tr>\r\n</tbody>\r\n</table>\r\n\r\n## Training\r\nVicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model's maximum context length.\r\n\r\nOur training recipe builds on top of [Stanfordâ€™s alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) with the following improvements.\r\n- **Memory Optimizations:** To enable Vicuna's understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing [gradient checkpointing](https://arxiv.org/abs/1604.06174) and [flash attention](https://arxiv.org/abs/2205.14135).\r\n- **Multi-round conversations:** We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot's output.\r\n- **Cost Reduction via Spot Instance:** The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ [SkyPilot](https://github.com/skypilot-org/skypilot) [managed spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html) to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.\r\n\r\n\r\n## Serving\r\nWe build a serving system that is capable of serving multiple models with distributed workers. It supports flexible plug-in of GPU workers from both on-premise clusters and the cloud. By utilizing a fault-tolerant controller and managed spot feature in SkyPilot, this serving system can work well with cheaper spot instances from multiple clouds to reduce the serving costs. It is currently a lightweight implementation and we are working on integrating more of our latest [research](https://arxiv.org/abs/2302.11665) into it.\r\n\r\n## How To Evaluate a Chatbot?\r\nEvaluating AI chatbots is a challenging task, as it requires examining language understanding, reasoning, and context awareness. With AI chatbots becoming more advanced, current open benchmarks may no longer suffice. For instance, the evaluation dataset used in Stanfordâ€™s Alpaca, [self-instruct](https://github.com/yizhongw/self-instruct/tree/main/human_eval), can be effectively answered by SOTA chatbots, making it difficult for humans to discern differences in performance. More limitations include training/test data contamination and the potentially high cost of creating new benchmarks. To tackle these issues, we propose an evaluation framework based on GPT-4 to automate chatbot performance assessment.\r\n\r\nFirst, we devised eight question categories, such as Fermi problems, roleplay scenarios, and coding/math tasks, to test various aspects of a chatbot's performance. Through careful prompt engineering, GPT-4 is able to generate diverse, challenging questions that baseline models struggle with. We select ten questions per category and collect answers from five chatbots: LLaMA, Alpaca, ChatGPT, Bard, and Vicuna. We then ask GPT-4 to rate the quality of their answers based on helpfulness, relevance, accuracy, and detail. We discover that GPT-4 can produce not only relatively consistent scores but also detailed explanations on why such scores are given (detailed examples [link](https://vicuna.lmsys.org/eval)). However, we also notice that GPT-4 is not very good at judging coding/math tasks.\r\n\r\n<img src=\"/images/blog/vicuna/response-compare.png\" style=\"display: flex; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 50%;\"></img>\r\n<p style=\"color:gray; text-align: center;\">Figure 3. Response Comparison Assessed by GPT-4</p>\r\n\r\nFigure 3 displays the comparison results between all baselines and Vicuna. GPT-4 prefers Vicuna over state-of-the-art open-source models (LLaMA, Alpaca) in more than 90% of the questions, and it achieves competitive performance against proprietary models (ChatGPT, Bard). In 45% of the questions, GPT-4 rates Vicuna's response as better or equal to ChatGPT's.\r\nAs GPT-4 assigns a quantitative score to each response on a scale of 10, we calculate the total score for each (baseline, Vicuna) comparison pair by adding up the scores obtained by each model on 80 questions. As shown in Table 2, Vicunaâ€™s total score is 92% of ChatGPTâ€™s. Despite recent advancements, these chatbots still face limitations, such as struggling with basic math problems or having limited coding ability.\r\n\r\n<p style=\"color:gray; text-align: center;\">Table 2. Total Scores Assessed by GPT-4. </p>\r\n\r\n<table class=\"tg\" style=\"display: flex;justify-content: center;\">\r\n<tbody>\r\n  <tr>\r\n    <td class=\"tg-head\"><span style=\"font-weight:bold;\">Baseline</span></td>\r\n    <td class=\"tg-head\"><span style=\"font-weight:bold;\">Baseline Score</span></td>\r\n    <td class=\"tg-head\"><span style=\"font-weight:bold;\">Vicuna Score</span></td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-body\">LLaMA-13B</td>\r\n    <td class=\"tg-body\" style=\"text-align: right\">513.0</td>\r\n    <td class=\"tg-body\" style=\"text-align: right\"><span style=\"font-weight:bold;\">694.0</span></td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-body\">Alpaca-13B</td>\r\n    <td class=\"tg-body\" style=\"text-align: right\">583.0</td>\r\n    <td class=\"tg-body\" style=\"text-align: right\"><span style=\"font-weight:bold;\">704.0</span></td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-body\">Bard</td>\r\n    <td class=\"tg-body\" style=\"text-align: right\"><span style=\"font-weight:bold;\">664.0</span></td>\r\n    <td class=\"tg-body\" style=\"text-align: right\">655.5</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-body\">ChatGPT</td>\r\n    <td class=\"tg-body\" style=\"text-align: right\"><span style=\"font-weight:bold;\">693.0</span></td>\r\n    <td class=\"tg-body\" style=\"text-align: right\">638.0</td>\r\n  </tr>\r\n</tbody>\r\n</table>\r\n<br>\r\n\r\nWhile this proposed evaluation framework demonstrates the potential for assessing chatbots, it is not yet a rigorous or mature approach, as large language models are prone to hallucinate. Developing a comprehensive, standardized evaluation system for chatbots remains an open question requiring further research.\r\n\r\n## Limitations\r\nWe have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI [moderation](https://platform.openai.com/docs/guides/moderation/overview) API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.\r\n\r\n## Release\r\nIn our first release, we will share the training, serving, and evaluation code on a GitHub repo: [https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat).\r\nWe also released the Vicuna-13B model weights, please find the instructions [here](https://github.com/lm-sys/FastChat#vicuna-weights).\r\nThere is no plan to release the dataset.\r\nJoin our [Discord](https://discord.gg/h6kCZb72G7) server and follow our [Twitter](https://twitter.com/lmsysorg) to get the latest updates.\r\n\r\n## License\r\nThe online demo is a research preview intended for non-commercial use only, subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us If you find any potential violation.\\\\\r\nThe code is released under the Apache License 2.0.\r\n\r\n## The Team\r\nThis is a joint effort with collaborators from multiple institutions, including UC Berkeley, CMU, Stanford, UC San Diego, and MBZUAI.\r\n\r\n**Students (alphabetical order):**\\\r\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang\r\n\r\n**Advisors (alphabetical order):**\\\r\nJoseph E. Gonzalez, Ion Stoica, Eric P. Xing\r\n\r\n\r\n## Acknowledgment\r\nWe would like to thank Xinyang Geng, Hao Liu, and Eric Wallace from BAIR; Xuecheng Li, and Tianyi Zhang from Stanford Alpaca team for their insightful discussion and feedback; Qirong Ho from MBZUAI for providing support on the serving cluster. Please check out a blog post from BAIR about a concurrent effort on their chatbot, [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/).\r\n\r\n\r\n## Citation\r\n```\r\n@misc{vicuna2023,\r\n    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\\%* ChatGPT Quality},\r\n    url = {https://vicuna.lmsys.org},\r\n    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},\r\n    month = {March},\r\n    year = {2023}\r\n}\r\n```\r\n","date":1680134400000}]},"__N_SSG":true}